<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Word Embedding with TensorFlow.js</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/black.css" id="theme">
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

    <!-- Custom Styling -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
    <style>
        * {
            font-family: 'Poppins', sans-serif !important;
        }
        body.reveal-viewport {
            background: linear-gradient( 45deg, #542907 0%, rgba(2, 0, 36, 1) 20%,rgba(2, 0, 36, 1) 20%, rgba(2, 0, 36, 1) 65%, rgba(66, 80, 102, 1) 100%);
        }
        
        .reveal h1,
        .reveal h2,
        .reveal h3,
        .reveal h4,
        .reveal h5,
        .reveal h6 {
            text-transform: none;
        }
        
        strong,
        a {
            color: #FF6F00;
        }
        .reveal strong, .reveal b {
            font-weight: 500;
        }        
        ul.small {
            font-size: 2rem;
        }
        
        ul.xs,
        strong.xs {
            font-size: 1.5rem;
        }
		table.small {
			font-size: 2rem;
		}
        p.small {
            font-size: 1.5rem;
            max-width: 60ch;
            margin: 0 auto;
            line-height: 2.2rem;
        }
		p.xs, span.xs {
			font-size: 1rem;
		}
        .border {
            border: 1px solid white;
        }
        .highlight {
            color: #ff6f02;
        }
		.semi {
			opacity: 0.5;
		}
		.bg-white {
			background-color: #fff !important;
		}
    </style>
</head>

<body>
    <div class="reveal">
        <div class="slides">

            <!-- INTRO -->
            <section>
	    	<section>
		    <p class="small">A Gentle Introduction to</p>
	            <h2>Word &amp; Sentence Embeddings<br>in <strong>Machine Learning</strong></h2>
		    <p class="small">By Kostas Minaidis</p>
	    	</section>
	    	<section>
			<h6>( king - man ) + woman = ?</h6>
	    	</section>

            </section>

            <!-- TECHNOLOGIES -->
            <section>
                <h2>Technologies:</h2>
                <ul>
                    <li class="fragment">
                        <strong>JavaScript</strong>
                        <ul class="xs">
                            <li class="fragment">(The Programming language)</li>
                        </ul>
                    </li>
                    <li class="fragment">
                        <strong>TensorFlow.JS</strong>
                        <ul class="xs">
                            <li class="fragment">(The Machine Learning Library)</li>
                        </ul>
                    </li>
                    <li class="fragment">
                        <strong>Universal Sentence Encoder</strong>
                        <ul class="xs">
                            <li class="fragment">(The Machine Learning Model)</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- OBJECTIVES -->
            <section>
                <h2>Objectives</h2>
                <ul class="small">
                    <li class="fragment">Understanding natural language with Machine Learning</li>
                    <li class="fragment">Encoding Text: Turning Words and Sentences into Vectors</li>
                    <li class="fragment">Word &amp; Sentence Embeddings</li>
                    <li class="fragment">Word Embeddings using <strong>Word2Vec</strong></li>
                    <li class="fragment">Introduction to the <strong>Universal Sentence Encoder</strong></li>
                    <li class="fragment">Improving <em>Question &amp; Answering Systems</em> using Sentence Similarity with the <strong>USE</strong> Model</li>
                </ul>
            </section>

            <!-- NLP with ML -->
            <section>
                <section>
                    <h6>Understanding Natural Language using</h6>
                    <h2><strong>Machine Learning</strong></h2>
                    <img src="img/undraw_Modern_life_re_8pdp.svg" width="250" alt="">
                </section>
                <section>
                    <h6>Processing and Understanding<br>Natural Language <em>(Pre-ML Era)</em></h6>
                    <ul class="xs">
                        <li class="fragment">Rule-based (grammar/syntactic): <strong><span class="fragment">if</span> <em class="fragment">this</em> <span class="fragment">then</span> <em class="fragment">that</em></strong></li>
                        <li class="fragment">Dictionary-based: <strong><em class="fragment">Hello</em> <span class="fragment">&rtrif;</span> <em class="fragment">你好</em></strong></li>
                        <li class="fragment">Statistical Models</li>
                        <li class="fragment">Limitations: Ambiguation, Mispelled words, Neologisms, Unseen text, Slang, Internet Parlance (OMG, LOL, etc.)</li>
                        <li class="fragment">Labor-intensive / Time-consuming</li>
                        <li class="fragment">Something was missing...</li>
                    </ul>
                </section>

                <section>
                    <h6>The Machine Learing Revolution</h6>
                    <ul class="xs">
                        <li class="fragment">Booming factors for ML:
                            <ul>
                                <li class="fragment">
                                    <strong>Large Datasets</strong>
                                </li>
                                <li class="fragment">
                                    <strong>Vast amounts of natural language content</strong>
                                </li>
                                <li class="fragment"><strong>Speed / Performance / Architectural Advantage / Rapid Growth</strong></li>
                                <li class="fragment">
                                    <strong>Impressive results in Pattern Recognition</strong>
                                </li>
                                <li class="fragment"><strong>Time for A.I. to take the lead...</strong></li>
                            </ul>
                        </li>
                    </ul>
                    <p class="fragment">
                        <strong class="xs">Read More<br>&dtrif;</strong>
                    </p>
                </section>

                <!-- NLP/NLU Pre-ML -->
                <section>
                    <p class="small">Before the advent and proliferation of Machine Learning in the early 2000's, the task of processing and understanding natural language using computers was mainly based on Statistical models and <em>Rule-based</em> systems, something closely related to traditional programming.</p>
                    <br>
                    <code class="fragment"><strong>if</strong> this word <strong>then</strong> that</code>
                </section>
                <section>
                    <p class="small">Basic rules were handcrafted according to the grammar and syntactic rules along with dictionary-based word translations.</p>
                </section>
                <section>
                    <p class="small">
                        This approach revealed early on various limitations such as making sense of and dealing with ambiguations, unseen text content, neologisms, slang and Internet parlance.
                    </p>
                    <p class="fragment small">It also proved to be a labor-intensive and time-consuming process.</p>
                </section>
                <!-- ML ERA -->
                <section>
                    <p class="small">As Machine Learning and more specifically <strong>Artificial Neural Networks</strong> started showing their amazing capabilities and rapid progress in the early 2000's, computer scientists working in the fields of NLP and NLU started
                        switching their focus to ML-based approaches for dealing with the growing challenges of these fields.</p>
                </section>
                <section>
                    <p class="small">The Machine Learning revolution in the early 2000's was driven by the accumulation of larger and larger datasets, especially of natural language content: from Wikipedia entries to social media posts and from news websites to digital messages.</p>
                    <br>
                    <p class="fragment small">At the same time the power, speed and architectural advantages of ML algorithms based on Neural Networks were revealing their amazing pattern recognition skills.</p>
                </section>
                <section>
                    <p class="small">Machine Learning was mature enough, highly efficient and provided substantial competitive advantages over traditional (rule-based) methods that dealt with NLP and NLU.</p>
                    <br>
                    <p class="fragment small">It was time for Artificial Intelligence to take the lead in areas such as these.</p>
                </section>
            </section>

            <!-- Encoding Text -->
            <section>
                <section>
                    <h2>Encoding Text:<br><strong>Turning Words into Vectors</strong></h2>
                </section>
				<section>
					<ul>
						<li>Computers work with numbers. Words must be converted to numbers.</li>
						<li class="fragment">Several Approaches exist: 
							<ul>
								<li class="fragment"><strong>Index-based Encoding</strong></li>
								<li class="fragment"><strong>One-Hot Encoding</strong></li>
								<li class="fragment"><strong>Word Embeddings</strong></li>
							</ul>
						</li>
					</ul>
				</section>
                <!-- RANDOM NUMBERS -->
                <section>
                    <h6>Assigning a random unique number<br>for each word</h6>
                    <table>
                        <tr>
                            <td>A</td>
                            <td>cat</td>
                            <td>and</td>
                            <td>a</td>
                            <td>dog</td>
                            <td>play</td>
                            <td>together</td>
                        </tr>
                        <tr class="border">
                            <td class="fragment border"><strong>1</strong></td>
                            <td class="fragment border"><strong>2</strong></td>
                            <td class="fragment border"><strong>3</strong></td>
                            <td class="fragment border"><strong>1</strong></td>
                            <td class="fragment border"><strong>4</strong></td>
                            <td class="fragment border"><strong>5</strong></td>
                            <td class="fragment border"><strong>6</strong></td>
                        </tr>
                    </table>
                </section>
                <!-- ONE HOT ENCODING -->
                <section>
                    <h6>One-Hot Encoding</h6>
                    <table class="small">
                        <tr class="border highlight">
                            <td class="border"></td>
                            <td class="border">a</td>
                            <td class="border">cat</td>
                            <td class="border">and</td>
                            <td class="border">dog</td>
                            <td class="border">play</td>
                            <td class="border">together</td>
                            <td>
                                <p class="small">
                                    <strong style="font-size: 1rem;">&ltrif; Vocabulary</strong>
                                </p>
                            </td>
                        </tr>
                        <tr class="border fragment">
                            <td class="border">a</td>
                            <td class="border highlight">1</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border"></td>
                        </tr>
                        <tr class="border fragment">
                            <td class="border">cat</td>
                            <td class="border">0</td>
                            <td class="border highlight">1</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border"></td>
                        </tr>
                        <tr class="border fragment">
                            <td class="border">and</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border highlight">1</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border"></td>
                        </tr>
                        <tr class="border fragment">
                            <td class="border">a</td>
                            <td class="border highlight">1</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border"></td>
                        </tr>
                        <tr class="border fragment">
                            <td class="border">dog</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border highlight">1</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border"></td>
                        </tr>
                        <tr class="border fragment">
                            <td class="border">play</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border highlight">1</td>
                            <td class="border">0</td>
                            <td class="border"></td>
                        </tr>
                        <tr class="border fragment">
                            <td class="border">together</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border">0</td>
                            <td class="border highlight">1</td>
                            <td class="border"></td>
                        </tr>
                    </table>
                    <p class="fragment">
                        <strong class="xs">
							Read more about their limitations
							<br><small class="semi"><em>(size, semantics)</em></small>
							<br>&dtrif;
						</strong>
                    </p>
				</section>
				<!-- READ MORE -->
                <section>
                    <p class="small">In order for computers (these digital, number-crunching machines) to be able to handle words and text, they have to be converted into, well... digits. Numbers.</p>
                    <br>
                    <p class="fragment small">The process of encoding text (characters, words, sentences) into numbers is as old as computers themselves.</p>
                </section>
                <section>
                    <p>Various methods can be used to convert words into numbers.</p>
					<ul class="small">
						<li class="fragment"><strong>Assigning a unique random number for each word</strong></li>
						<li class="fragment"><strong>One-Hot Encoding</strong></li>
						<li class="fragment"><strong>Word Embeddings</strong></li>
					</ul>
                </section>
				<section>
					<p class="small">
						The first two methods (Index-based and One-Hot Encoding) have some serious disadvantages and limitations.
					</p>
                    <br>
					<p class="fragment small">One-Hot encoding creates huge vectors with size as large as the vocabulary itself. If the vocabulary consists of 100,000 unique words, then we have 100,000 vectors each in the size of 100,000 numbers.</p>
                    <br>
					<p class="fragment small">That's pretty huge.</p>
				</section>
				<section>
					<p>Most importantly, both methods lack one significant aspect: <span class="fragment">capturing the <strong>meaning</strong> of these words.</span></p>
					<p class="fragment">The <strong>semantics</strong> of these words are not reflected in these numbers.</p>
				</section>
                <section>
                    <p class="small">Take for example the following one-hot encoded vectors of two (semantically) similar words:</p>
                    <br>
                    <p class="small"><strong>Hotel</strong></p>
                    <table class="border small">
                        <tr>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td><strong>1</strong></td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                        </tr>
                    </table>
                    <p class="small"><strong>Motel</strong></p>
                    <table class="border small">
                        <tr class="border">
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td><strong>1</strong></td>
                            <td>0</td>
                        </tr>
                    </table>
                    <br>
                    <p class="fragment small">These vector representations do not reflect the semantic similarity of these two words and therefore cannot be used as a similarity metric.</p>
                </section>

            </section>

			<!-- Word Embeddings -->
			<section>
				<section>
					<h2>Word Embeddings</h2>
				</section>
				<!-- What are Word Embeddings? -->
				<section>
					<h6>What are Word Embeddings?</h6>
					<ul class="small">
						<li class="fragment"><strong>A light-weight (and unique) numerical representation (vectors)</strong></li>
						<li class="fragment">
							<strong>Capture the words' relationship and meaning (semantics)</strong>
							<ul class="fragment xs">
								<li class="fragment">The numbers actually represent something about the word's meaning!</li>
                                <li class="fragment"><span class="semi">(Mention: Orange/Apple Analogy)</span></li>
							</ul>
						</li>
						<li class="fragment">
							<strong>These word vectors live in a semantic vector space</strong>
							<ul class="xs">
								<li class="fragment">Each word/vector is projected into this vector space according to its semantic relationship with other words (context).</li>
								<li class="fragment">We can visualize this space and actually <strong>see</strong> the semantic relationships.</li>
								<li class="fragment">We can apply vector arithmetic on these words achieve useful results!</li>
							</ul>
						</li>
					</ul>
				</section>
				<!-- RECAPPING VECTORS -->
				<section>
					<p>Vectors: Quick Recap (1/2)</p>
					<p class="xs"><a href="https://www.wikiwand.com/en/Euclidean_vector">Source: Wikipedia</a></p>
					<img class="bg-white" src="img/Vector_from_A_to_B.svg.png" alt="">
					<p class="small">
						<em>&quot;A geometric object that has magnitude (or length) and direction.&quot; </em>
					</p>
					<p class="small fragment">
						<em>&quot;Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors&quot;</em>
					</p>
				</section>
                <section>
                    <p>Vectors: Quick Recap (2/2)</p>
                    <p class="small fragment">
                        A vector can be expressed as an array of numbers. 
                    </p>
                    <p class="small fragment"><strong>[ 0.5, 1.2, 3.0, 1.6, 4.1 ]</strong></p>
                    <p class="small fragment">A vector of size 2x1 contains 2 numbers and can be thought of as a point in 2D space. In the same manner a vector of size 3x1 is a point in 3D space, a vector of 5x1 a point in 5D space and so on. </p>
                    <p class="small fragment">We can calculate <strong>how close of further apart two vectors are</strong> based on the distance measure between them.</p>
                </section>
				<section>
					<h6>Embeddings: Words as Vectors</h6>
					<img class="bg-white" src="img/Word_Vectors.jpg" alt="">
                    <p class="xs">
                        <small class="semi">(Image: Robert Meyer)</small>
                    </p>
				</section>
				<section>
					<h6>Embeddings: Words as Vectors</h6>
					<img class="bg-white" src="img/Semantic_Relations_in_Vector_Space.png" alt="">
				</section>
				<section class="iframe">
					<h6>Vector Spaces:<br>Word Embeddings from Sentiment Analysis</h6>
				</section>
				<section data-background-iframe="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/kostasx/e327e1b8cece75e40a5a0548b9aca101/raw/0dd906c044506322f21478ec43de70b8c7e1e896/embedding-projector.config.json" data-background-interactive>
				</section>
                <section>
					<p class="small">
						<a target="_blank" href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/kostasx/e327e1b8cece75e40a5a0548b9aca101/raw/0dd906c044506322f21478ec43de70b8c7e1e896/embedding-projector.config.json">Click to open the <strong>Embedding Projector</strong> in a new Page.</a>
					</p>
					<p class="xs semi">
						<a href="https://projector.tensorflow.org/">https://projector.tensorflow.org/</a>
					</p>
                </section>
                <!-- USE CASES -->
				<section>
					<h6>Use cases?</h6>
					<ul class="small">
						<li class="fragment">
                            Semantic Similarity: synonyms, antonyms, recommendations, etc.
                            <ul class="xs fragment">
                                <li><em class="">I am looking for a <strong>laptop</strong> battery</em> <span class="fragment">&rtrif;</span> <em class="fragment">I am looking for a <strong>notebook</strong> battery</em></li>
                            </ul>
                        </li>
						<li class="fragment">
							<a href="img/book_embeddings.png" target="_blank">Classification</a> &amp; <a href="img/clustering.png" target="_blank">Semantic Clustering</a> (by topic, gender, etc.)
							<small class="semi"><a href="https://devopedia.org/word-embedding"><em>(Source: Devopedia)</em></a></small>
						</li>
						<li class="fragment">Visualization in 2D/3D space</li>
						<li class="fragment">Language Translation: <a href="img/cross-lingual--word-embeddings__luong_et_al_2015.jpg" target="_blank">Cross-lingual Word Embeddings</a></li>
                        <li class="fragment"><a target="_blank" href="https://kostasx.github.io/Word2Vec-in-JavaScript/"><strong>Demo time!</strong></a></li>
					</ul>
					<p class="fragment">
						<strong class="xs">
							Read More
							<br>&dtrif;
						</strong>
					</p>
				</section>
                <!-- READ MORE -->
				<section>
					<p class="small">Word Embeddings provide a new approach, revolutionizing the way words are turned into numbers.</p>
                    <br>
					<p class="fragment small">
						Each word is not only encoded into a unique and light-weight vector, but this numerical representation gets to capture the word's semantics.</p>
                    <br>
					<p class="fragment small">
						In simple words, <strong>these numbers encode</strong> (or one might say, reflect) <strong>the word's meaning</strong>.
					</p>
				</section>
				<section>
					<p><em>&quot;You shall know a word<br>by the company it keeps&quot;<br>J.R.Firth</em></p>
					<p class="fragment small">Word embeddings are created by Machine Learning algorithms that are trained on a large corpus of text.</p>
                    <br>
					<p class="fragment small">During the training, the algorithm learns who to represent each word as a point and place it in the vector space according to the words that surround it.</p>
				</section>
				<section>
					<p>Popular Word Embedding Implementations:</p>
					<ul class="small">
						<li class="fragment"><a href="https://www.wikiwand.com/en/Word2vec">Word2Vec</a></li>
						<li class="fragment"><a href="https://fasttext.cc/">FastText</a></li>
						<li class="fragment"><a href="https://nlp.stanford.edu/projects/glove/">GloVe</a></li>
					</ul>
				</section>
				<!-- REFERENCES -->
				<section>
					<h6>References</h6>
					<ul class="xs">
						<li><a href="https://www.usna.edu/Users/oceano/raylee/SM223/Ch12_2_Stewart(2016).pdf">More about Vectors</a></li>
                        <li><a href="https://www.youtube.com/watch?v=4-QoMdSqG_I">Intuition & Use-Cases of Embeddings in NLP & beyond</a></li>
						<li><a href="https://www.wikiwand.com/en/Word_embedding">Word Embeddings on Wikipedia</a></li>
						<li><a href="https://www.youtube.com/watch?v=ERibwqs9p38">Word Vector Representations: word2vec</a></li>
						<li><a href="https://medium.com/@khanbaykaner/a-quick-report-on-cross-lingual-word-embeddings-4e671885d543">Cross-lingual Word Embeddings</a></li>
						<li><a href="https://ruder.io/cross-lingual-embeddings/">A survey of cross-lingual word embedding models</a></li>
                        <li><a href="http://turbomaze.github.io/word2vecjson/">Word2Vec demo</a></li>
                        <li><a href="http://archive.aueb.gr:7000/">Word Vectors in Greek Language</a></li>
					</ul>
				</section>
			</section>

			<!-- Sentence & Document Embeddings -->
			<section>
				<section>
					<h6>More Embeddings:</h6>
					<h2>Sentence &amp; Document Embeddings</h2>
				</section>

				<section>
					<p class="small">Following the success and popularity of Word Embeddings, similar algorithms were applied for both text sentences and documents which yielded equally successful results.</p>
                    <img width="70%" src="img/Sentence_Embeddings.png" alt="">
				</section>

                <!-- Universal Sentence Encoder -->
                <section>
                    <h6>Universal Sentence Encoder</h6>
                    <p class="fragment small"><em>&quot;The Universal Sentence Encoder (Cer et al., 2018) (USE) is a model that encodes text into 512-dimensional embeddings.&quot;</em></p>
                    <br>
                    <p class="fragment small"><em>&quot;These embeddings can then be used as inputs to natural language processing tasks such as <strong>sentiment classification</strong> and <strong>textual similarity analysis</strong>.&quot;</em></p>
                </section>

                <section>
                    <h6>Universal Sentence Encoder<br><strong>with TensorFlow.js</strong></h6>
                    <p class="small fragment">A lightweight version <em>(lite)</em> of a pre-trained USE model is available for use with TensorFlow.JS in any JavaScript environment (browser/server).</p>
                    <br>
                    <p class="small fragment">The model can be found in the official TensorFlow.js models page <a href="https://www.tensorflow.org/js/models">here</a> or directly in the <a href="https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder">models repository</a>.</p>
                </section>

                <!-- USE Demo #1 -->
                <section>
                    <h6>
                        <strong>Demo time:</strong>
                    </h6>
                    <h6 class="fragment">
                        Calculating text similarity with USE model.
                    </h6>
                    <p class="small fragment semi"><a target="_blank" href="https://github.com/kostasx/Discover-Embeddings-with-TensorFlow.js/tree/demo">Code (demo branch)</a></p>
                </section>

                <!-- USE Demo #2 -->
                <section>
                    <h6>
                        <strong>Demo time:</strong>
                    </h6>
                    <h6 class="fragment">Predicting Users Questions</h6>
                    <p class="small fragment semi">
                        <a target="_blank" href="https://kostasx.github.io/Discover-Embeddings-with-TensorFlow.js/tfjs-use-questions.html">Live Demo</a><a target="_blank" href="https://github.com/kostasx/Discover-Embeddings-with-TensorFlow.js/blob/main/tfjs-use-questions.html"> | Code</a>
                    </p>
                    <br>
                    <p class="fragment small">Things to try. Add the following questions:</p>
                    <br>
                    <ul class="small fragment">
                        <li>How old are you?</li>
                        <li>What is your favorite color?</li>
                        <li>When should I change my phone battery?</li>
                    </ul>
                </section>
                
                <!-- USE Demo #3 -->
                <section>
                    <h6>
                        <strong>Demo time:</strong>
                    </h6>
                    <h6 class="fragment">Real-life Use Case:<br>Improving Question Answering Systems</h6>
                    <p class="small fragment semi">
                        <a target="_blank" href="https://kostasx.github.io/Discover-Embeddings-with-TensorFlow.js/tfjs-embeddings.html">Live Demo</a><a target="_blank" href="https://github.com/kostasx/Discover-Embeddings-with-TensorFlow.js/blob/main/tfjs-embeddings.html"> | Code</a>
                    </p>
                    <br>
                    <h6 class="small fragment">Improvements?</h6>
                    <p class="small fragment">Use a Paraphrasing model to add more questions to the mix: 
                        <br><a href="https://colab.research.google.com/github/ramsrigouthamg/Paraphrase-any-question-with-T5-Text-To-Text-Transfer-Transformer-/blob/master/t5-pretrained-question-paraphraser.ipynb">Google Colab</a>
                        <a href="https://colab.research.google.com/drive/1485nyRfkydg9cYWziOQCoDQfVQK6lGKW?usp=sharing"> | Google Colab with FAQ</a>
                    </p>
                </section>

                <!-- USE Demo #4 -->
                <section>
                    <h6>
                        <strong>Demo time:</strong>
                    </h6>
                    <h6 class="fragment">Toxicity Checker</h6>
                    <p class="small fragment semi">
                        <a target="_blank" href="https://kostasx.github.io/Discover-Embeddings-with-TensorFlow.js/tfjs-toxicity.html">Live Demo</a><a target="_blank" href="https://github.com/kostasx/Discover-Embeddings-with-TensorFlow.js/blob/main/tfjs-toxicity.html"> | Code</a><a href="https://codepen.io/kostasx/full/YzWpYXb"> | Codepen</a>
                    </p>
                    <p class="fragment">
						<strong class="xs">
							Read More
							<br>&dtrif;
						</strong>
					</p>					
                </section>

                <!-- WHERE TO GO FROM HERE? -->
                <section>
                    <h2>What's next?</h2>
                    <ul class="xs">
                        <li class="fragment">Try the USE <a href="https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder#universal-sentence-encoder-for-question-answering">QnA model</a></li>
                        <li class="fragment">Create desktop applications (using Electron.JS) for text manipulation and natural language processing (Grammarly?)</li>
                        <li class="fragment">Train your own models on any language you want and on any topic you like.</li>
                        <li class="fragment">Run the model on the server using Node.JS and serve the predictions using a JSON API</li>
                        <li class="fragment">Run full-size models on the server (overcome browser size limitations)</li>
                    </ul>
                </section>

                <!-- REFERENCES -->
				<section>
					<h6>References:</h6>
					<ul class="xs">
						<li><a href="https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/">An Essential Guide to Pretrained Word Embeddings for NLP Practitioners</a></li>
                        <li><a href="https://engineering.talkdesk.com/what-are-word-embeddings-and-why-are-they-useful-a45f49edf7ab">What are Word Embeddings and why are they useful?</a></li>
                        <li><a href="https://blog.floydhub.com/automate-customer-support-part-one/#word-embeddings-the-power-of-context">Word embeddings: the power of context</a></li>
					</ul>
				</section>
			</section>
        </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
        // More info about initialization & config:
        // - https://revealjs.com/initialization/
        // - https://revealjs.com/config/
        Reveal.initialize({
            hash: true,

            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
        });
    </script>
</body>

</html>
